# -*- coding: utf-8 -*-
"""Augmented Resized Data RGB Kelp and Fish Yolo-V5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n6-R7SXA0QbtILUdrGo98N5qV5ASvTta
"""

from PIL import Image
import cv2

"""##Setting Up The YOLOv5 Environment"""

!git clone https://github.com/ultralytics/yolov5  # clone repo
!pip install -U -r yolov5/requirements.txt  # install dependencies

#installing for google colab GPU use
!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/yolov5

!ls

import torch
from IPython.display import Image  # for displaying images
# from utils.google_utils import gdrive_download  # for downloading models/datasets

print('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))

"""### Downloading the data"""

# You need to sign up in roboflow to get the key and then you can use the dataset
!curl -L "https://app.roboflow.com/ds/YRDeArnNYZ?key=7zCCi2Zm26" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

"""The dataset got downloaded inside the yolov5 folder. 

Path to train dataset - /content/yolov5/train 

Path to test dataset - /content/yolov5/test

## Define Model Configuration and Architecture
"""

# Commented out IPython magic to ensure Python compatibility.
# %cat data.yaml

"""Path to the train and test dataset is not correct. We need to edit it in the next steps"""

# define number of classes based on YAML
# data.yaml contains the information about number of classes and their labels required for this project
import yaml
with open("data.yaml", 'r') as stream:
    num_classes = str(yaml.safe_load(stream)['nc'])

#customize iPython writefile so we can write variables
from IPython.core.magic import register_line_cell_magic

@register_line_cell_magic
def writetemplate(line, cell):
    with open(line, 'w') as f:
        f.write(cell.format(**globals()))

# Commented out IPython magic to ensure Python compatibility.
# # Below we are changing the data configuration for right path to the dataset
# %%writetemplate /content/yolov5/data.yaml
# 
# train: ./train/images
# val: ./valid/images
# 
# nc: 2
# names: ['FishRaysKelp', 'Kelp']

# Commented out IPython magic to ensure Python compatibility.
#Let's check the data.yaml file for confirmation

# %cat data.yaml

with open(r'data.yaml') as file:
    # The FullLoader parameter handles the conversion from YAML
    # scalar values to Python the dictionary format
    labels_list = yaml.load(file, Loader=yaml.FullLoader)

    label_names = labels_list['names']

print("Number of Classes are {}, whose labels are {} for this Object Detection project".format(num_classes,label_names))

# Commented out IPython magic to ensure Python compatibility.
#this is the model configuration we will use for our tutorial 
# yolov5s.yaml contains the configuration of neural network required for training.
# %cat /content/yolov5/models/yolov5l.yaml

# Commented out IPython magic to ensure Python compatibility.
# # Below we are changing the configuration so that it becomes compatible to number of classes required in this project
# %%writetemplate /content/yolov5/models/custom_yolov5l.yaml
# 
# # Parameters
# nc: num_classses  # number of classes
# depth_multiple: 1.0  # model depth multiple
# width_multiple: 1.0  # layer channel multiple
# anchors:
#   - [10,13, 16,30, 33,23]  # P3/8
#   - [30,61, 62,45, 59,119]  # P4/16
#   - [116,90, 156,198, 373,326]  # P5/32
# 
# # YOLOv5 v6.0 backbone
# backbone:
#   # [from, number, module, args]
#   [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
#    [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
#    [-1, 3, C3, [128]],
#    [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
#    [-1, 6, C3, [256]],
#    [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
#    [-1, 9, C3, [512]],
#    [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
#    [-1, 3, C3, [1024]],
#    [-1, 1, SPPF, [1024, 5]],  # 9
#   ]
# 
# # YOLOv5 v6.0 head
# head:
#   [[-1, 1, Conv, [512, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [[-1, 6], 1, Concat, [1]],  # cat backbone P4
#    [-1, 3, C3, [512, False]],  # 13
# 
#    [-1, 1, Conv, [256, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [[-1, 4], 1, Concat, [1]],  # cat backbone P3
#    [-1, 3, C3, [256, False]],  # 17 (P3/8-small)
# 
#    [-1, 1, Conv, [256, 3, 2]],
#    [[-1, 14], 1, Concat, [1]],  # cat head P4
#    [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)
# 
#    [-1, 1, Conv, [512, 3, 2]],
#    [[-1, 10], 1, Concat, [1]],  # cat head P5
#    [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)
# 
#    [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
#   ]

"""## Train Custom YOLOv5 Detector

### Next, we'll fire off training!Â¶
Here, we are able to pass a number of arguments:

* **img:** define input image size
* **batch:** determine batch size
* **epochs:** define the number of training epochs. 
* **data:** set the path to our yaml file
* **cfg:** specify our model configuration
* **weights:** specify a custom path to weights. 
* **name:** result names
* **nosave:** only save the final checkpoint
* **cache:** cache images for faster training




"""

import os
os.chdir('/content/yolov5')

!pip install wandb
import wandb

# Commented out IPython magic to ensure Python compatibility.
# # train yolov5l on object detection data for 100 epochs [aroung 1000 epochs for better training and result]
# # NOTE: All the images are already pre-processed to 416 x 416 size.
# # We will be training for 100 epoch (increase it for better result) with batch size of 80
# # data.yaml also contains the information about location of Train and Validation Data. That's how you get the train data.
# # the training also requires the configuration of neural network, which is in custom_yolov5s.yaml
# # weights will be by-default stored at /content/yolov5/runs/exp2/weights/best.pt
# # time its performance
# %%time
# %cd /content/yolov5/
# !python train.py --img 416 --batch 64 --epochs 100 --data './data.yaml' --cfg ./models/custom_yolov5l.yaml --weights '' --device 0

"""## Evaluate Custom YOLOv5 Detector Performance
Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5l_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.

Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`
"""

# Commented out IPython magic to ensure Python compatibility.
# Start tensorboard
# Launch after you have started training to all the graphs needed for inspection
# logs save in the folder "runs"
# %load_ext tensorboard
# %tensorboard --logdir /content/yolov5/runs

"""## Curious? Visualize Our Training Data with Labels
After training starts, view `train*.jpg` images to see training images, labels and augmentation effects.

Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4.](https://arxiv.org/abs/2004.10934)


"""

# first, display our ground truth data
# The ground truth [Train data] is available in jpg file at location /content/yolov5/runs/train/exp2/test_batch0_labels.jpg 
print("GROUND TRUTH TRAINING DATA:")
im = Image.open('/content/yolov5/runs/train/exp2/val_batch0_labels.jpg')
Image.Image.show(im)

# print out an augmented training example
# Below is the augmented training data.
# NOTE: The dataset already contains the augmented data with annotations, so that you dont have to do it.
print("GROUND TRUTH AUGMENTED TRAINING DATA:")
im = Image.open('/content/yolov5/runs/train/exp2/train_batch0.jpg')
Image.Image.show(im)

"""# Run Inference With Trained Weights
Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow.
"""

# Commented out IPython magic to ensure Python compatibility.
# use the best weights!
# Final weights will be by-default stored at /content/yolov5/runs/train/exp5/weights/best.pt
# %cd /content/yolov5/
!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 416 --conf 0.4 --source ./test/images

"""### Check the output"""

#display inference on ALL test images
#this looks much better with longer training above

import glob
from IPython.display import Image, display

for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG
    display(Image(filename=imageName))
    print("\n")

"""## Export Trained Weights for Future Inference"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')

# %cp /content/yolov5/runs/train/exp2/weights/best.pt /content/gdrive/My\ Drive

"""# Split Test Video into Frames and Run Inference with Trained Weights"""

vidObj = cv2.VideoCapture('/content/gdrive/My Drive/KelpandFish_short.mp4')

numFrames = 0
success = 1

while success:
  success, image = vidObj.read()

  if(success == 1):
    newImg = cv2.resize(image, (416, 416))
    cv2.imwrite("/content/gdrive/MyDrive/FishKelpFrames/frame%s.jpg" % str(numFrames).zfill(4), newImg)
    numFrames += 1

# Commented out IPython magic to ensure Python compatibility.
# use the best weights!
# Final weights will be by-default stored at /content/yolov5/runs/train/exp2/weights/best.pt
# %cd /content/yolov5/
!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 416 --conf 0.4 --source /content/gdrive/MyDrive/FishKelpFrames

import glob
from IPython.display import Image, display

for imageName in glob.glob('/content/yolov5/runs/detect/exp5/*.jpg'): #assuming JPG
    display(Image(filename=imageName))
    print("\n")

"""# Create Video from Detected Images"""

frameSize = (416, 416)
out = cv2.VideoWriter('/content/gdrive/MyDrive/predictionVideo.avi', cv2.VideoWriter_fourcc(*'DIVX'), 60, frameSize)

for filename in glob.glob('/content/yolov5/runs/detect/exp5/*.jpg'):
  img = cv2.imread(filename)
  out.write(img)

out.release()

"""# Export Predictions"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
# drive.mount('/content/gdrive')

# %cp -r /content/yolov5 /content/gdrive/MyDrive/Augmented\ Resized\ YOLOv5

"""# Try With A Different Video"""



"""# Try With Yet One More Video"""

